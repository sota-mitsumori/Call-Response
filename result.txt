  0%|          | 0/12 [00:00<?, ?it/s]
/opt/miniconda3/envs/callresponse/lib/python3.10/site-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/var/folders/1b/gvcxsgys679c56xf5qc8kq9h0000gn/T/ipykernel_5366/4207896577.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403226260/work/torch/csrc/utils/tensor_new.cpp:278.)
  batch_mask = torch.tensor(mask_list, dtype=torch.float32, device=device)
0/12 | Loss: 7.940293 | 4.742430, 5.434772, 2.712217, 1.711072, 4.584793, 3.487069, 4.829752
  8%|▊         | 1/12 [00:59<10:58, 59.89s/it]
1/12 | Loss: 6.673228 | 3.813344, 4.247586, 1.997353, 0.832455, 4.569431, 3.098693, 4.679370
 17%|█▋        | 2/12 [04:28<24:35, 147.51s/it]
2/12 | Loss: 5.944417 | 3.009916, 3.337332, 1.870485, 0.831047, 4.305063, 2.868166, 4.513022
 25%|██▌       | 3/12 [08:24<28:08, 187.67s/it]
3/12 | Loss: 5.385541 | 2.324681, 2.666664, 1.687931, 0.810862, 4.262573, 2.710723, 4.360603
 33%|███▎      | 4/12 [18:16<46:19, 347.39s/it]
4/12 | Loss: 5.063503 | 2.065205, 2.411430, 1.685024, 0.847697, 4.100104, 2.599566, 4.072678
 42%|████▏     | 5/12 [21:38<34:25, 295.10s/it]
5/12 | Loss: 4.737877 | 1.768463, 2.075567, 1.663471, 0.816741, 4.013179, 2.532423, 3.830820
 50%|█████     | 6/12 [25:18<26:57, 269.56s/it]
6/12 | Loss: 4.512064 | 1.567641, 1.809139, 1.601376, 0.806081, 3.896291, 2.301658, 3.690968
 58%|█████▊    | 7/12 [28:01<19:33, 234.62s/it]
7/12 | Loss: 4.287124 | 1.480174, 1.697854, 1.692660, 0.807396, 3.667346, 2.238256, 3.414869
 67%|██████▋   | 8/12 [32:26<16:17, 244.28s/it]
8/12 | Loss: 4.113119 | 1.335177, 1.583395, 1.666529, 0.790374, 3.476475, 2.142140, 3.412488
 75%|███████▌  | 9/12 [35:32<11:17, 225.94s/it]
9/12 | Loss: 3.919990 | 1.160221, 1.440814, 1.552501, 0.774239, 3.400986, 2.030225, 3.287166
 83%|████████▎ | 10/12 [38:44<07:11, 215.71s/it]
10/12 | Loss: 3.755123 | 0.954517, 1.241985, 1.442388, 0.741055, 3.476633, 2.026474, 3.380581
 92%|█████████▏| 11/12 [43:16<03:52, 232.77s/it]
11/12 | Loss: 3.689611 | 0.875644, 1.189360, 1.535289, 0.762414, 3.484812, 2.121113, 3.195385
100%|██████████| 12/12 [1:13:44<00:00, 368.74s/it]
------------------------------------
epoch: 1/5 | Loss: 5.0018242200215655 | time: 1:13:44.866081 | batches_ok: 12/12
 [*] saving model to ./exp, name: loss_high_epoch_0

  0%|          | 0/12 [00:00<?, ?it/s]
0/12 | Loss: 3.541519 | 0.743523, 1.093144, 1.439790, 0.747074, 3.199582, 1.985071, 3.183506
  8%|▊         | 1/12 [02:57<32:33, 177.56s/it]
1/12 | Loss: 3.431230 | 0.568942, 0.933158, 1.280981, 0.693550, 3.371836, 1.977018, 3.340547
 17%|█▋        | 2/12 [06:29<32:56, 197.63s/it]
2/12 | Loss: 3.277850 | 0.522235, 0.894832, 1.269280, 0.714085, 3.146564, 1.736918, 3.283634
 25%|██▌       | 3/12 [08:55<26:05, 173.95s/it]
3/12 | Loss: 3.205020 | 0.422663, 0.697381, 1.131768, 0.687285, 3.264328, 1.827230, 3.259530
 33%|███▎      | 4/12 [11:20<21:42, 162.85s/it]
4/12 | Loss: 3.126839 | 0.450148, 0.798900, 1.260865, 0.744214, 3.065507, 1.799808, 2.814043
 42%|████▏     | 5/12 [14:26<19:57, 171.14s/it]
5/12 | Loss: 3.031525 | 0.341029, 0.661826, 1.300632, 0.710162, 3.024765, 1.825780, 2.751773
 50%|█████     | 6/12 [18:17<19:09, 191.57s/it]
6/12 | Loss: 3.015814 | 0.365505, 0.691497, 1.273282, 0.720721, 3.057775, 1.632331, 2.765865
 58%|█████▊    | 7/12 [20:52<14:56, 179.39s/it]
7/12 | Loss: 2.934023 | 0.359105, 0.683229, 1.408111, 0.740560, 2.869030, 1.604786, 2.552162
 67%|██████▋   | 8/12 [27:31<16:37, 249.28s/it]
8/12 | Loss: 2.858841 | 0.326201, 0.641367, 1.406396, 0.740294, 2.714448, 1.553025, 2.515364
 75%|███████▌  | 9/12 [30:28<11:20, 226.68s/it]
9/12 | Loss: 2.763304 | 0.308783, 0.634121, 1.300616, 0.732511, 2.711044, 1.462580, 2.404343
 83%|████████▎ | 10/12 [33:33<07:07, 213.84s/it]
10/12 | Loss: 2.712698 | 0.266342, 0.572841, 1.227334, 0.708226, 2.862624, 1.542729, 2.638016
 92%|█████████▏| 11/12 [36:57<03:30, 210.81s/it]
11/12 | Loss: 2.910830 | 0.257439, 0.581181, 1.339155, 0.756046, 3.135469, 1.722997, 2.651734
100%|██████████| 12/12 [39:50<00:00, 199.25s/it]
------------------------------------
epoch: 2/5 | Loss: 3.0674577554066977 | time: 1:53:37.348024 | batches_ok: 12/12
 [*] saving model to ./exp, name: loss_high_epoch_1

  0%|          | 0/12 [00:00<?, ?it/s]
0/12 | Loss: 2.772635 | 0.245213, 0.582078, 1.242617, 0.740015, 2.593395, 1.534751, 2.713926
  8%|▊         | 1/12 [01:57<21:30, 117.29s/it]
1/12 | Loss: 2.744488 | 0.190346, 0.552776, 1.103043, 0.652681, 2.863896, 1.612399, 2.897841
 17%|█▋        | 2/12 [04:18<21:52, 131.20s/it]
2/12 | Loss: 2.674104 | 0.227872, 0.573631, 1.087437, 0.693090, 2.577864, 1.370390, 3.001666
 25%|██▌       | 3/12 [06:59<21:43, 144.84s/it]
3/12 | Loss: 2.650094 | 0.202107, 0.426997, 0.962114, 0.686664, 2.703645, 1.513108, 2.909814
 33%|███▎      | 4/12 [09:34<19:51, 148.94s/it]
4/12 | Loss: 2.550059 | 0.281821, 0.591208, 1.092019, 0.715007, 2.522662, 1.527362, 2.359272
 42%|████▏     | 5/12 [12:03<17:22, 148.95s/it]
5/12 | Loss: 2.480480 | 0.201228, 0.467961, 1.145132, 0.676039, 2.465185, 1.580913, 2.310281
 50%|█████     | 6/12 [14:12<14:12, 142.04s/it]
6/12 | Loss: 2.631021 | 0.273063, 0.534167, 1.116470, 0.698224, 2.638277, 1.403778, 2.300033
 58%|█████▊    | 7/12 [15:59<10:54, 130.82s/it]
7/12 | Loss: 2.585763 | 0.285767, 0.551025, 1.250572, 0.710926, 2.493743, 1.390654, 2.164216
 67%|██████▋   | 8/12 [17:59<08:28, 127.14s/it]
8/12 | Loss: 2.481032 | 0.263288, 0.525022, 1.258320, 0.695173, 2.435877, 1.333663, 2.136520
 75%|███████▌  | 9/12 [20:16<06:30, 130.25s/it]
9/12 | Loss: 2.419820 | 0.256173, 0.521473, 1.161520, 0.683896, 2.486696, 1.256647, 2.053239
 83%|████████▎ | 10/12 [22:25<04:20, 130.02s/it]
10/12 | Loss: 2.391836 | 0.224069, 0.473274, 1.086724, 0.668317, 2.660575, 1.359873, 2.316467
 92%|█████████▏| 11/12 [24:50<02:14, 134.66s/it]
11/12 | Loss: 2.679949 | 0.222629, 0.513678, 1.195316, 0.674163, 3.028886, 1.564899, 2.475988
100%|██████████| 12/12 [27:10<00:00, 135.85s/it]
------------------------------------
epoch: 3/5 | Loss: 2.588439921538035 | time: 2:20:48.011048 | batches_ok: 12/12
 [*] saving model to ./exp, name: loss_high_epoch_2

  0%|          | 0/12 [00:00<?, ?it/s]
0/12 | Loss: 2.528439 | 0.216752, 0.481240, 1.116181, 0.690079, 2.404269, 1.367196, 2.522818
  8%|▊         | 1/12 [02:10<23:52, 130.23s/it]
1/12 | Loss: 2.527067 | 0.162367, 0.468114, 0.978122, 0.615470, 2.718792, 1.466161, 2.711597
 17%|█▋        | 2/12 [04:35<23:11, 139.15s/it]
2/12 | Loss: 2.483035 | 0.207221, 0.505805, 0.986567, 0.606975, 2.421234, 1.264939, 2.842391
 25%|██▌       | 3/12 [06:56<21:00, 140.07s/it]
3/12 | Loss: 2.457973 | 0.185041, 0.379074, 0.886742, 0.631476, 2.528544, 1.408723, 2.744700
 33%|███▎      | 4/12 [09:36<19:43, 147.96s/it]
4/12 | Loss: 2.342227 | 0.264745, 0.535059, 1.021010, 0.590495, 2.342260, 1.380138, 2.218275
 42%|████▏     | 5/12 [11:54<16:48, 144.09s/it]
5/12 | Loss: 2.296931 | 0.185877, 0.420261, 1.056922, 0.604472, 2.296308, 1.474949, 2.180121
 50%|█████     | 6/12 [14:12<14:13, 142.24s/it]
6/12 | Loss: 2.423631 | 0.259520, 0.474356, 1.022384, 0.690223, 2.483207, 1.292141, 2.146458
 58%|█████▊    | 7/12 [16:14<11:17, 135.41s/it]
7/12 | Loss: 2.393756 | 0.276285, 0.509369, 1.150860, 0.685908, 2.364988, 1.295784, 2.026792
 67%|██████▋   | 8/12 [18:15<08:43, 130.97s/it]
8/12 | Loss: 2.351007 | 0.255640, 0.482996, 1.172440, 0.654856, 2.342281, 1.253353, 2.047616
 75%|███████▌  | 9/12 [20:36<06:42, 134.16s/it]
9/12 | Loss: 2.307603 | 0.247785, 0.479031, 1.096418, 0.619164, 2.398427, 1.178603, 2.002655
 83%|████████▎ | 10/12 [22:48<04:26, 133.47s/it]
10/12 | Loss: 2.287364 | 0.218505, 0.436299, 1.019423, 0.668658, 2.570971, 1.289990, 2.242699
 92%|█████████▏| 11/12 [25:10<02:16, 136.08s/it]
11/12 | Loss: 2.592955 | 0.218874, 0.508168, 1.142329, 0.654534, 3.018585, 1.514839, 2.375164
100%|██████████| 12/12 [28:29<00:00, 142.45s/it]
------------------------------------
epoch: 4/5 | Loss: 2.415998876094818 | time: 2:49:17.939095 | batches_ok: 12/12
 [*] saving model to ./exp, name: loss_high_epoch_3

  0%|          | 0/12 [00:00<?, ?it/s]
0/12 | Loss: 2.445384 | 0.211057, 0.445362, 1.068253, 0.655645, 2.359735, 1.319974, 2.439739
  8%|▊         | 1/12 [02:03<22:34, 123.17s/it]
1/12 | Loss: 2.450791 | 0.154860, 0.435413, 0.933436, 0.601720, 2.658819, 1.422014, 2.639668
 17%|█▋        | 2/12 [04:22<22:07, 132.74s/it]
2/12 | Loss: 2.396947 | 0.200556, 0.480146, 0.949934, 0.592843, 2.342627, 1.214474, 2.744860
 25%|██▌       | 3/12 [06:22<19:02, 126.92s/it]
3/12 | Loss: 2.384188 | 0.176860, 0.354062, 0.852017, 0.597666, 2.451303, 1.383612, 2.672909
 33%|███▎      | 4/12 [08:25<16:43, 125.41s/it]
4/12 | Loss: 2.269512 | 0.255654, 0.513087, 0.993759, 0.582714, 2.274784, 1.357853, 2.170282
 42%|████▏     | 5/12 [10:49<15:24, 132.11s/it]
5/12 | Loss: 2.237590 | 0.179691, 0.392628, 1.023642, 0.614951, 2.245870, 1.427526, 2.146179
 50%|█████     | 6/12 [13:01<13:11, 131.94s/it]
6/12 | Loss: 2.342012 | 0.255246, 0.455700, 0.994016, 0.687131, 2.448795, 1.283306, 2.114967
 58%|█████▊    | 7/12 [14:57<10:33, 126.64s/it]
7/12 | Loss: 2.326149 | 0.269774, 0.495623, 1.109826, 0.661094, 2.332737, 1.274046, 2.016455
 67%|██████▋   | 8/12 [17:06<08:30, 127.52s/it]
8/12 | Loss: 2.271423 | 0.247071, 0.473081, 1.125527, 0.650764, 2.288570, 1.227504, 1.981307
 75%|███████▌  | 9/12 [19:17<06:26, 128.74s/it]
9/12 | Loss: 2.213410 | 0.234648, 0.464332, 1.056136, 0.619734, 2.330405, 1.150866, 1.928017
 83%|████████▎ | 10/12 [21:16<04:11, 125.56s/it]
10/12 | Loss: 2.204642 | 0.209058, 0.429428, 0.973246, 0.676543, 2.494991, 1.271742, 2.163692
 92%|█████████▏| 11/12 [23:16<02:03, 123.93s/it]
11/12 | Loss: 2.521985 | 0.211412, 0.518865, 1.095053, 0.686173, 2.944493, 1.476064, 2.292476
100%|██████████| 12/12 [25:41<00:00, 128.48s/it]
------------------------------------
epoch: 5/5 | Loss: 2.3386694192886353 | time: 3:15:00.127150 | batches_ok: 12/12
 [*] saving model to ./exp, name: loss_high_epoch_4